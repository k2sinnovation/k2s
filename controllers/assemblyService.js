const WebSocket = require("ws");

// Stocker les sockets GPT par deviceId pour streaming multiple
const gptSockets = new Map(); // Map<deviceId, WebSocket GPT temps r√©el>

/**
 * Envoie un chunk audio PCM √† GPT et commit si demand√©
 * @param {string} deviceId - ID du device Flutter
 * @param {string} audioBase64 - chunk audio PCM Base64
 * @param {Map} wsClients - Map<deviceId, { ws: WebSocket }>
 * @param {boolean} commit - true si c'est le dernier chunk du segment
 */
async function processAudioChunk(deviceId, audioBase64, wsClients, commit = false) {
  console.log(`[Assembly][${deviceId}] üì• R√©ception chunk (longueur: ${audioBase64.length} chars, commit: ${commit})`);
  
  // Nettoyage du Base64
  const base64Data = audioBase64.includes(",") ? audioBase64.split(",")[1] : audioBase64;
  const audioBuffer = Buffer.from(base64Data, "base64");
  console.log(`[Assembly][${deviceId}] üîÑ Buffer cr√©√©: ${audioBuffer.length} bytes`);

  // Cr√©er socket GPT si n'existe pas
  if (!gptSockets.has(deviceId)) {
    console.log(`[Assembly][${deviceId}] üÜï Cr√©ation nouvelle connexion GPT...`);
    
    const wsGPT = new WebSocket(
      "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01",
      { 
        headers: { 
          "Authorization": `Bearer ${process.env.OPENAI_API_KEY}`,
          "OpenAI-Beta": "realtime=v1"
        } 
      }
    );

    let responseText = "";

    wsGPT.on("open", () => {
      console.log(`[GPT][${deviceId}] ‚úÖ Connexion WebSocket ouverte`);
      
      // Configuration de la session
      wsGPT.send(JSON.stringify({
        type: "session.update",
        session: {
          modalities: ["text", "audio"],
          instructions: "Tu es un assistant vocal intelligent. R√©ponds de mani√®re concise et naturelle.",
          voice: "alloy",
          input_audio_format: "pcm16",
          output_audio_format: "pcm16",
          input_audio_transcription: {
            model: "whisper-1"
          },
          turn_detection: {
            type: "server_vad",
            threshold: 0.5,
            prefix_padding_ms: 300,
            silence_duration_ms: 500
          }
        }
      }));
      console.log(`[GPT][${deviceId}] ‚öôÔ∏è Session configur√©e`);
    });

    wsGPT.on("message", (data) => {
      let msg;
      try { 
        msg = JSON.parse(data.toString()); 
      } catch (e) { 
        console.warn(`[GPT][${deviceId}] ‚ö†Ô∏è Erreur parsing message:`, e.message); 
        return; 
      }

      // Log de tous les messages pour debug
      console.log(`[GPT][${deviceId}] üì® Type re√ßu: ${msg.type}`);

      // R√©cup√©ration du client Flutter
      const clientData = wsClients.get(deviceId);
      if (!clientData || !clientData.ws) {
        console.warn(`[GPT][${deviceId}] ‚ö†Ô∏è Client Flutter non trouv√© ou d√©connect√©`);
        return;
      }
      const wsClient = clientData.ws;

      if (wsClient.readyState !== WebSocket.OPEN) {
        console.warn(`[GPT][${deviceId}] ‚ö†Ô∏è WebSocket Flutter pas ouvert (state: ${wsClient.readyState})`);
        return;
      }

      // === Gestion Transcription Audio Input ===
      if (msg.type === "conversation.item.input_audio_transcription.completed") {
        console.log(`[GPT][${deviceId}] üé§ Transcription: "${msg.transcript}"`);
        wsClient.send(JSON.stringify({
          type: 'input_transcription',
          deviceId,
          transcript: msg.transcript,
          index: Date.now(),
        }));
      }

      // === Gestion Texte Incr√©mental ===
      if (msg.type === "response.text.delta") {
        const delta = msg.delta || '';
        responseText += delta;
        console.log(`[GPT][${deviceId}] üìù Texte delta: "${delta}"`);
        
        wsClient.send(JSON.stringify({
          type: 'response.output_audio_transcript.delta',
          deviceId,
          delta: delta,
          index: Date.now(),
        }));
      }

      // === Gestion Audio PCM Incr√©mental ===
      if (msg.type === "response.audio.delta") {
        const audioChunk = msg.delta;
        console.log(`[GPT][${deviceId}] üîä Audio delta re√ßu: ${audioChunk ? audioChunk.length : 0} chars`);
        
        wsClient.send(JSON.stringify({
          type: 'response.output_audio.delta',
          deviceId,
          delta: audioChunk,
          index: Date.now(),
        }));
      }

      // === Gestion Audio Transcript ===
      if (msg.type === "response.audio_transcript.delta") {
        console.log(`[GPT][${deviceId}] üì¢ Audio transcript: "${msg.delta}"`);
        wsClient.send(JSON.stringify({
          type: 'response.output_audio_transcript.delta',
          deviceId,
          delta: msg.delta,
          index: Date.now(),
        }));
      }

      // === Fin de R√©ponse ===
      if (msg.type === "response.done") {
        console.log(`[GPT][${deviceId}] ‚úÖ R√©ponse compl√®te (texte: "${responseText}")`);
        
        wsClient.send(JSON.stringify({
          type: 'response.completed',
          deviceId,
          fullText: responseText,
          index: Date.now(),
        }));
        
        responseText = ""; // Reset pour prochaine r√©ponse
      }

      // === Erreurs ===
      if (msg.type === "error") {
        console.error(`[GPT][${deviceId}] ‚ùå Erreur GPT:`, msg.error?.message || JSON.stringify(msg));
        wsClient.send(JSON.stringify({
          type: 'gpt_error',
          deviceId,
          error: msg.error?.message || 'Erreur inconnue',
          index: Date.now(),
        }));
      }

      // === Session Update ===
      if (msg.type === "session.created" || msg.type === "session.updated") {
        console.log(`[GPT][${deviceId}] ‚öôÔ∏è Session: ${msg.type}`);
      }
    });

    wsGPT.on("close", () => {
      console.log(`[GPT][${deviceId}] üîå Connexion WebSocket ferm√©e`);
      gptSockets.delete(deviceId);
    });

    wsGPT.on("error", (err) => {
      console.error(`[GPT][${deviceId}] ‚ùå Erreur WebSocket:`, err.message);
      gptSockets.delete(deviceId);
    });

    gptSockets.set(deviceId, wsGPT);
    
    // Attendre que la connexion soit √©tablie avant d'envoyer
    await new Promise((resolve) => {
      if (wsGPT.readyState === WebSocket.OPEN) {
        resolve();
      } else {
        wsGPT.once('open', resolve);
      }
    });
  }

  // Envoyer chunk audio √† GPT
  const wsGPT = gptSockets.get(deviceId);
  
  if (!wsGPT || wsGPT.readyState !== WebSocket.OPEN) {
    console.error(`[Assembly][${deviceId}] ‚ùå Socket GPT non disponible ou ferm√©`);
    return;
  }

  const audioPayload = {
    type: "input_audio_buffer.append",
    audio: audioBuffer.toString("base64"),
  };
  
  wsGPT.send(JSON.stringify(audioPayload));
  console.log(`[Assembly][${deviceId}] üì§ Chunk envoy√© √† GPT (${audioBuffer.length} bytes)`);

  // Commit + cr√©ation r√©ponse si c'est le dernier chunk
  if (commit) {
    console.log(`[Assembly][${deviceId}] üèÅ Commit du buffer audio...`);
    
    wsGPT.send(JSON.stringify({ 
      type: "input_audio_buffer.commit" 
    }));
    
    console.log(`[Assembly][${deviceId}] üéØ Cr√©ation de la r√©ponse...`);
    
    wsGPT.send(JSON.stringify({ 
      type: "response.create",
      response: {
        modalities: ["text", "audio"],
        instructions: "R√©ponds de mani√®re naturelle et concise √† ce qui vient d'√™tre dit."
      }
    }));
    
    console.log(`[Assembly][${deviceId}] ‚úÖ R√©ponse cr√©√©e, en attente de la g√©n√©ration GPT...`);
  }
}

module.exports = { processAudioChunk };
